# Databricks notebook source
# MAGIC %md
# MAGIC ### Welcome to the first part of the Databricks GenAI Workshop!
# MAGIC #### This workshop will be split into two sections.
# MAGIC #### Section 1 will cover the basics of Prompt Engineering. You'll learn about:
# MAGIC - Calling models from the Foundation Model API and Langchain
# MAGIC - Creating simple prompt templates that execute before a user's query
# MAGIC - Differences in Zero-Shot vs Multi-Shot prompting
# MAGIC - Explain Model's Outputs with Chain of Thought Prompting
# MAGIC - Reducing Hallucinations with Confidence Check Prompt
# MAGIC - Registering Models/Chains with MLflow and Unity Catalog
# MAGIC
# MAGIC Credit to Debu Sinha from which this was largely adapted from: https://github.com/debu-sinha/Databricks-GenAI-Series

# COMMAND ----------

# DBTITLE 1,Install Libraries
#Install libraries
%pip install mlflow==2.10.0 lxml==4.9.3 langchain==0.0.344 databricks-vectorsearch==0.22 cloudpickle==2.2.1 databricks-sdk==0.12.0 cloudpickle==2.2.1 pydantic==2.5.2 datasets

# COMMAND ----------

# DBTITLE 1,Import Libraries and User Setup
#Import Libraries
from transformers import pipeline
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
from transformers.utils import logging

#setup catalog and show widget at top
dbutils.widgets.text("catalog_name","main")
catalog_name = dbutils.widgets.get("catalog_name")

#break user in their own schema
current_user = spark.sql("SELECT current_user() as username").collect()[0].username
schema_name = f'genai_workshop_{current_user.split("@")[0].split(".")[0]}'

#create schema
spark.sql(f"CREATE SCHEMA IF NOT EXISTS {catalog_name}.{schema_name}")
print(f"\nUsing catalog + schema: {catalog_name}.{schema_name}")

# COMMAND ----------

# MAGIC %md
# MAGIC Prompt engineering is a critical aspect of interacting effectively with Large Language Models (LLMs). It involves crafting prompts that can be appended to user inputs that guide the model to generate the most relevant and accurate outputs. This skill is valuable for several reasons:
# MAGIC #
# MAGIC 1. **Precision and Relevance**: Well-engineered prompts help the model understand the context and specificity of the query, leading to more precise and relevant responses.
# MAGIC 2. **Efficiency**: Effective prompts can reduce the number tokens generated by the model, while still maintaining accuracy/correctness. This saves time and computational resources.
# MAGIC 3. **Creative and Complex Tasks**: For tasks that require creativity or complex problem-solving, carefully designed prompts can significantly improve the quality of the model's output.

# COMMAND ----------

# MAGIC %md
# MAGIC <img src="https://daxg39y63pxwu.cloudfront.net/images/blog/langchain/LangChain.webp" alt="LangChain" width="700"/>.   
# MAGIC ## LangChain 
# MAGIC LangChain is a framework designed to simplify the creation of applications using large language models. It enables applications that:
# MAGIC     - Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.)
# MAGIC     - Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.) language models.

# COMMAND ----------

# DBTITLE 1,Check Langchain Version
import langchain
print(langchain.__version__)

# COMMAND ----------

# DBTITLE 1,Define Model (DBRX)
import os
from langchain.llms import HuggingFacePipeline
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain.chat_models import ChatDatabricks

#call dbrx, hosted by Databricks Foundation Model API
dbrx_model = ChatDatabricks(endpoint="databricks-dbrx-instruct", max_tokens = 450)

# COMMAND ----------

# DBTITLE 1,Start with a Simple Question about Spark Joins
user_question = "How can I speed up my Spark join operation?"
dbrx_model.predict(user_question)

# COMMAND ----------

# DBTITLE 1,Creating a Prompt Template to Append to our Inputs
from langchain import PromptTemplate
from langchain.chains import LLMChain

#now, let's create a prompt template to make our incoming queries databricks-specific
intro_template = """
You are a Databricks support engineer tasked with answering questions about Spark. Include Databricks-relevant information in your response and be as prescriptive as possible. Cite Databricks documentation for your answers
User Question:" {question}"
"""

# COMMAND ----------

# DBTITLE 1,Create an LLM Chain that appends our Template to the User Input
#Create a prompt template 
prompt_template = PromptTemplate(
    input_variables=["question"],
    template=intro_template,
)

#Create a chain that ties together our template and model
dbrx_chain = LLMChain(
    llm=dbrx_model,
    prompt=prompt_template,
    output_key="Support Response",
    verbose=False
)

dbrx_chain_response = dbrx_chain.run({"question":user_question})
print(dbrx_chain_response)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Zero-Shot Prompting
# MAGIC Don't provide any examples to the model, and just ask the question
# MAGIC ## Few Shot Prompting
# MAGIC Provide N examples to the model to help guide it's response

# COMMAND ----------

# DBTITLE 1,Helper Function to our LLM Chains
#This just saves us from repeating the steps of creating Prompt Templates and the LLMChain
def run_llm_chain(input_string, template_string, model):
  """
  given an input string, template, and model, execute a langchain chain on the input with a given prompt template

  params
  ==========
  input_string (str): the incoming query from a user to be evaluated
  template_string (str): prompt template append or pre-pend to input_string (required for prompt engineering)
  model (langchain model): the name of the model 
  """
  prompt_template = PromptTemplate(
    input_variables=["input_string"],
    template=template_string,
  )
  model_chain = LLMChain(
    llm=model,
    prompt=prompt_template,
    output_key="Response",
    verbose=False
  )

  return model_chain.run({"input_string": input_string})

# COMMAND ----------

#Example of a zero-shot prompt
zero_shot_template = """For each tweet, describe its sentiment:
                        [Tweet]: {input_string}
                      """

#Example of a multi-shot prompt which gives extra context
few_shot_template = """For each tweet, describe its sentiment:
                        [Tweet]: "I hate it when my phone battery dies."
                        [Sentiment]: Negative
                        ###
                        [Tweet]: "Screen broke again, I'm personally keeping ufixit in business"
                        [Sentiment]: Negative
                        ###
                        [Tweet]: "This is the link to the article"
                        [Sentiment]: Neutral
                        ###
                        [Tweet]: {input_string}
                        [Sentiment]:
                      """

# COMMAND ----------

# DBTITLE 1,View results of Zero Shot Prompt
#Prompt to provide sentiment for just "My day has been ugh" with no other context

tweet = "My day has been meh"
zero_shot_response = run_llm_chain(tweet, zero_shot_template, dbrx_model)
print(zero_shot_response)

# COMMAND ----------

# DBTITLE 1,View Results of Multi-Shot Prompt
#With multi-shot, we're giving it extra context. Including previous tweets where they say they're not having a great day.

few_shot_response = run_llm_chain(tweet, few_shot_template, dbrx_model)
print(few_shot_response)

#Now you see the sentiment isn't neutral but rather neutral or slightly negative since it has extra context to base off of

# COMMAND ----------

# MAGIC %md
# MAGIC ## Chain of Thought Prompting
# MAGIC Chain of thought prompting makes the LLM step through it's decision-making process. This makes it easier to better understand model outputs or identify hallucinations

# COMMAND ----------

# DBTITLE 1,Chain of Thought Example
apples = """
"Imagine you are at a grocery store and need to buy apples. They are sold in bags of 6 apples each and cost $2 per bag. If you need 20 apples for a recipe, how many bags should you buy and how much will it cost?
"""

dbrx_model.predict(apples)

# COMMAND ----------

# DBTITLE 1,Chain of Thought Explained
chain_of_reasoning_prompt = """
                            For the following question, answer the question, but walk through your line of reasoning step by step to arrive at the answer:

                            {input_string}
                            """

cor_response = run_llm_chain(apples, chain_of_reasoning_prompt, dbrx_model)
print(cor_response)

# COMMAND ----------

# MAGIC %md
# MAGIC # Less Hallucinations!
# MAGIC Finally, you can tell your model not to hallucinate. LLMs typically return a confidence score for their output sequence, so you can explicitly tell the model not to respond if it feels it does not have enough information to answer

# COMMAND ----------

# DBTITLE 1,Prompt to Reduce Hallucination
no_hallucinations_prompt = """
                            For the following question, only respond if you are absolutely certain you have sufficient information to generate a confident answer. If you cannot do so, then only respond 'Sorry - I don't have enough information to answer that.' 

                            Question:
                            {input_string}
                            """

# COMMAND ----------

# DBTITLE 1,Prompt for Unknown Information
#This question will be important later!
random_prompt = "What is ARES"

#ARES is a very specific evaluation framework for RAG applications and was released in a very recent paper: https://arxiv.org/abs/2311.09476
#This information almost certainly did not exist in the training set for the model, so it has no knowledge of it (yet!)
run_llm_chain(random_prompt, no_hallucinations_prompt, dbrx_model)

# COMMAND ----------

# DBTITLE 1,Prompt for Known Information
#Delta Live Tables however has been out for a couple years, so the model should know and provide us an answer!
liquid_cluster = "What is Delta Live Tables on Databricks?"
run_llm_chain(liquid_cluster, no_hallucinations_prompt, dbrx_model)

# COMMAND ----------

# MAGIC %md
# MAGIC # Try It Yourself!
# MAGIC Below, we set up a few LangChain prompts.
# MAGIC
# MAGIC If you need help look at cell #11 for reference.

# COMMAND ----------


#when crafting your prompt, remember that you need to add a variable injection (commonly with {}) to the template. See examples above.
#once you have the prompt + variable, add the variable build your PromptTemplate in cell 25 below
your_prompt_template_here = """
                            """

# COMMAND ----------

your_question_here = """
                     """

# COMMAND ----------

dbrx_model.predict(your_question_here)

# COMMAND ----------

your_prompt_template = PromptTemplate(
    #TODO
  )

your_model_chain = LLMChain(
  #TODO
)

model_chain.run() #TODO: what do we need to pass into run()?

# COMMAND ----------

# MAGIC %md
# MAGIC # Saving our LangChain LLM Chain to MLflow
# MAGIC This will set you up for model tracking and deployment down the line.

# COMMAND ----------

# DBTITLE 1,Set up Signature (Example inputs for our model)
from mlflow.models import infer_signature

input_str="How can I speed up my Spark joins?"
prediction = dbrx_chain.run(input_str)
input_columns = [
    {"type": "string", "name": input_key} for input_key in dbrx_chain.input_keys
]
signature = infer_signature(input_columns, prediction)

# COMMAND ----------

# DBTITLE 1,Create Experiment and Log Model to Run
import mlflow
import cloudpickle

# Create a new mlflow experiment or get the existing one if already exists.
current_user = spark.sql("SELECT current_user() as username").collect()[0].username
experiment_name = f"/Users/{current_user}/genai-prompt-engineering-workshop"
mlflow.set_experiment(experiment_name)

# set the name of our model
model_name = "dbrx_chain"

# get experiment id to pass to the run
experiment_id = mlflow.get_experiment_by_name(experiment_name).experiment_id
with mlflow.start_run(experiment_id=experiment_id):
    mlflow.langchain.log_model(
        dbrx_chain,
        model_name,
        signature=signature,
        input_example=input_str,
        pip_requirements=[
            "mlflow==" + mlflow.__version__,
            "langchain==" + langchain.__version__,
            "databricks-vectorsearch",
            "pydantic==2.5.2 --no-binary pydantic",
            "cloudpickle==" + cloudpickle.__version__
        ]
    )

# COMMAND ----------

# DBTITLE 1,Register Model into Unity Catalog
import mlflow

#grab our most recent run (which logged the model) using our experiment ID
runs = mlflow.search_runs([experiment_id])
last_run_id = runs.sort_values('start_time', ascending=False).iloc[0].run_id

#grab the model URI that's generated from the run
model_uri = f"runs:/{last_run_id}/{model_name}"

#log the model to catalog.schema.model. The schema name referenced below is generated for you in the init script
catalog = dbutils.widgets.get("catalog_name")
schema = schema_name

#set our registry location to Unity Catalog
mlflow.set_registry_uri("databricks-uc")
mlflow.register_model(
    model_uri=model_uri,
    name=f"{catalog}.{schema}.{model_name}"
)

# COMMAND ----------


